{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install XlsxWriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE6uMTp4trSa",
        "outputId": "861827a5-bc48-42cc-b5b3-8cfb1dc6eb50"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: XlsxWriter in /usr/local/lib/python3.11/dist-packages (3.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "pPvN3c9Dd33T"
      },
      "outputs": [],
      "source": [
        "# === Imports ===\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import copy\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "# === Utility Functions ===\n",
        "\n",
        "def safe_get(data, path, default=\"\"):\n",
        "    \"\"\"Safely get a nested value from a dictionary using a list of keys.\"\"\"\n",
        "    for key in path:\n",
        "        if isinstance(data, dict) and key in data:\n",
        "            data = data[key]\n",
        "        else:\n",
        "            return default\n",
        "    return default if data is None else data\n",
        "\n",
        "def extract_file_segment(item):\n",
        "    try:\n",
        "        images = item.get(\"content\", {}).get(\"imageComponent\", {}).get(\"images\", [])\n",
        "        for img in images:\n",
        "            for attr in img.get(\"attributes\", []):\n",
        "                vector = attr.get(\"detailData\", {}).get(\"vectorImage\", {})\n",
        "                for artifact in vector.get(\"artifacts\", []):\n",
        "                    if \"fileIdentifyingUrlPathSegment\" in artifact:\n",
        "                        return artifact[\"fileIdentifyingUrlPathSegment\"]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return \"\"\n",
        "\n",
        "def extract_epoch_from_segment(segment):\n",
        "    try:\n",
        "        parts = segment.split(\"/\")\n",
        "        if len(parts) >= 4:\n",
        "            timestamp_str = parts[3].split(\"?\")[0]\n",
        "            if timestamp_str.isdigit():\n",
        "                return int(timestamp_str)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def extract_entity_id(urn_str):\n",
        "    match = re.search(r'\\d{6,}', urn_str or \"\")\n",
        "    return int(match.group()) if match else None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load HAR file ===\n",
        "\"\"\"filename = \"/content/drive/MyDrive/LinkedInData/www.linkedin.com.har\"\n",
        "\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    har_data = json.load(f) \"\"\"\n",
        "\n",
        "\n",
        "# === Load ALL HAR files ===\n",
        "har_folder = \"/content/drive/MyDrive/LinkedInData\"\n",
        "har_files = glob.glob(f\"{har_folder}/*.har\")\n",
        "\n",
        "print(f\"✅ Found {len(har_files)} HAR files\")\n",
        "\n",
        "# Now load them one by one\n",
        "all_entries = []\n",
        "\n",
        "for filename in har_files:\n",
        "    print(f\"Loading: {filename}\")\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        har_data = json.load(f)\n",
        "        # Extract entries from this file and append\n",
        "        entries = har_data.get(\"log\", {}).get(\"entries\", [])\n",
        "        all_entries.extend(entries)\n",
        "\n",
        "# Now all_entries contains all entries from all HAR files\n",
        "print(f\"✅ Total entries loaded: {len(all_entries)}\")\n",
        "\n",
        "# OPTIONAL: If your extract_posts_and_social() expects har_data,\n",
        "# you can fake one like this:\n",
        "combined_har_data = {\"log\": {\"entries\": all_entries}}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuUJxJKkeDkH",
        "outputId": "704c3945-7c71-4014-e453-18ab28fc2885"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Found 10 HAR files\n",
            "Loading: /content/drive/MyDrive/LinkedInData/www.linkedin.com10.har\n",
            "Loading: /content/drive/MyDrive/LinkedInData/www.linkedin.com8.har\n",
            "Loading: /content/drive/MyDrive/LinkedInData/www.linkedin.com9.har\n",
            "Loading: /content/drive/MyDrive/LinkedInData/www.linkedin.com6.har\n",
            "Loading: /content/drive/MyDrive/LinkedInData/www.linkedin.com7.har\n",
            "Loading: /content/drive/MyDrive/LinkedInData/www.linkedin.com4.har\n",
            "Loading: /content/drive/MyDrive/LinkedInData/www.linkedin.com5.har\n",
            "Loading: /content/drive/MyDrive/LinkedInData/www.linkedin.com1.har\n",
            "Loading: /content/drive/MyDrive/LinkedInData/www.linkedin.com2.har\n",
            "Loading: /content/drive/MyDrive/LinkedInData/www.linkedin.com3.har\n",
            "✅ Total entries loaded: 25756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Extract posts & social data ===\n",
        "\n",
        "def extract_posts_and_social(har_data):\n",
        "    posts = []\n",
        "    social_data = []\n",
        "\n",
        "    for entry in har_data.get(\"log\", {}).get(\"entries\", []):\n",
        "        url = entry.get(\"request\", {}).get(\"url\", \"\")\n",
        "        if not url.startswith(\"https://www.linkedin.com/voyager/api/graphql?\"):\n",
        "            continue\n",
        "\n",
        "        text_data = entry.get(\"response\", {}).get(\"content\", {}).get(\"text\", \"\")\n",
        "        try:\n",
        "            response_json = json.loads(text_data)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "        for item in response_json.get(\"included\", []):\n",
        "            # === Posts ===\n",
        "            if item.get(\"$type\") == \"com.linkedin.voyager.dash.feed.Update\":\n",
        "                attributes = safe_get(item, [\"actor\", \"name\", \"attributesV2\"], [])\n",
        "                actor_profile = safe_get(attributes[0], [\"detailData\", \"*profileFullName\"]) if attributes else \"\"\n",
        "                actor_profile_id = actor_profile.split(\"profile:\")[1] if \"profile:\" in actor_profile else \"\"\n",
        "\n",
        "                file_segment = extract_file_segment(item)\n",
        "                image_epoch = extract_epoch_from_segment(file_segment)\n",
        "                image_datetime = datetime.fromtimestamp(image_epoch / 1000).strftime('%Y-%m-%d %H:%M:%S') if image_epoch else \"\"\n",
        "\n",
        "                entity_urn = safe_get(item, [\"metadata\", \"backendUrn\"])\n",
        "                header = safe_get(item, [\"header\", \"text\", \"text\"])\n",
        "\n",
        "                header_attributes = safe_get(item, [\"header\", \"text\", \"attributesV2\"], [])\n",
        "                header_profile = safe_get(header_attributes[0], [\"detailData\", \"*profileFullName\"]) if header_attributes else \"\"\n",
        "                header_profile_id = header_profile.split(\"profile:\")[1] if \"profile:\" in header_profile else \"\"\n",
        "\n",
        "                commentary_attributes = safe_get(item, [\"commentary\", \"text\", \"attributesV2\"], [])\n",
        "                company_id = safe_get(commentary_attributes[0], [\"detailData\", \"*companyName\"]) if commentary_attributes else None\n",
        "\n",
        "                socialDetail = safe_get(item, [\"*socialDetail\"])\n",
        "                resharedUpdate = safe_get(item, [\"*resharedUpdate\"])\n",
        "\n",
        "                resharedUpdate_id = extract_entity_id(resharedUpdate)\n",
        "                entity_id = extract_entity_id(entity_urn)\n",
        "\n",
        "                matches = re.findall(r'urn:li:(?:activity|ugcPost):(\\d+)\\b|urn:li:groupPost:\\d+-(\\d+)\\b', socialDetail)\n",
        "                flattened_matches = [m[0] or m[1] for m in matches if m[0] or m[1]]\n",
        "                sd_left = flattened_matches[0] if len(matches) >= 2 else None\n",
        "                sd_right = flattened_matches[1] if len(matches) >= 2 else None\n",
        "\n",
        "                post_data = {\n",
        "                    \"entity_id\": str(entity_id) if entity_id else \"\",\n",
        "                    \"resharedUpdate_id\": str(resharedUpdate_id) if resharedUpdate_id else \"\",\n",
        "                    \"header\": header,\n",
        "                    \"post_text\": safe_get(item, [\"commentary\", \"text\", \"text\"]),\n",
        "                    \"actor_description\": safe_get(item, [\"actor\", \"description\", \"text\"]),\n",
        "                    \"actor_name\": safe_get(item, [\"actor\", \"name\", \"text\"]),\n",
        "                    \"actor_profile\": actor_profile_id,\n",
        "                    \"actor_backendUrn\": safe_get(item, [\"actor\", \"backendUrn\"]),\n",
        "                    \"share_url\": safe_get(item, [\"socialContent\", \"shareUrl\"]),\n",
        "                    \"file_segment\": file_segment,\n",
        "                    \"image_epoch\": image_epoch,\n",
        "                    \"image_datetime\": image_datetime,\n",
        "                    \"socialDetail\": socialDetail,\n",
        "                    \"sd_left\": sd_left,\n",
        "                    \"sd_right\": sd_right,\n",
        "                    \"header_profile_id\": header_profile_id,\n",
        "                    \"company_id\": company_id\n",
        "                }\n",
        "\n",
        "                posts.append(post_data)\n",
        "\n",
        "            # === Social Data ===\n",
        "            elif item.get(\"$type\") == \"com.linkedin.voyager.dash.feed.SocialActivityCounts\":\n",
        "                entity_urn = safe_get(item, [\"entityUrn\"])\n",
        "                entity_id = extract_entity_id(entity_urn)\n",
        "\n",
        "                numReactions = safe_get(item, [\"numLikes\"])\n",
        "                numComments = safe_get(item, [\"numComments\"])\n",
        "                numShares = safe_get(item, [\"numShares\"])\n",
        "\n",
        "                reactionTypeCounts = safe_get(item, [\"reactionTypeCounts\"], [])\n",
        "                reaction_map = {r.get(\"reactionType\"): r.get(\"count\", 0) for r in reactionTypeCounts}\n",
        "\n",
        "                sd = {\n",
        "                    \"entity_id\": entity_id,\n",
        "                    \"numReactions\": numReactions,\n",
        "                    \"numLikes\": reaction_map.get(\"LIKE\", 0),\n",
        "                    \"numInterests\": reaction_map.get(\"INTEREST\", 0),\n",
        "                    \"numAppreciates\": reaction_map.get(\"APPRECIATION\", 0),\n",
        "                    \"numEntertains\": reaction_map.get(\"ENTERTAINMENT\", 0),\n",
        "                    \"numEmpathys\": reaction_map.get(\"EMPATHY\", 0),\n",
        "                    \"numPraises\": reaction_map.get(\"PRAISE\", 0),\n",
        "                    \"numComments\": numComments,\n",
        "                    \"numShares\": numShares\n",
        "                }\n",
        "\n",
        "                social_data.append(sd)\n",
        "\n",
        "    return posts, social_data\n",
        "\n",
        "\"\"\"\n",
        "# Run extraction\n",
        "posts, social_data = extract_posts_and_social(har_data)\n",
        "print(f\"✅ Extracted {len(posts)} posts and {len(social_data)} social records\")\"\"\"\n",
        "\n",
        "# Now run as normal:\n",
        "posts, social_data = extract_posts_and_social(combined_har_data)\n",
        "print(f\"✅ Extracted {len(posts)} posts and {len(social_data)} social records from ALL files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxmZKtB0eJHs",
        "outputId": "108a5720-a9b9-47e0-dcf5-f0f036ec2608"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extracted 3831 posts and 3817 social records from ALL files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Deduplicate posts ===\n",
        "\n",
        "def deduplicate_posts(posts):\n",
        "    best_posts = {}\n",
        "\n",
        "    for post in posts:\n",
        "        post_id = str(post.get(\"entity_id\", \"\"))\n",
        "        if not post_id:\n",
        "            continue\n",
        "\n",
        "        if post_id not in best_posts:\n",
        "            best_posts[post_id] = copy.deepcopy(post)\n",
        "        else:\n",
        "            current_best = best_posts[post_id]\n",
        "            for key, value in post.items():\n",
        "                if key not in current_best:\n",
        "                    current_best[key] = value\n",
        "                elif current_best[key] in [None, \"\", [], {}] and value not in [None, \"\", [], {}]:\n",
        "                    current_best[key] = value\n",
        "\n",
        "    return list(best_posts.values())\n",
        "\n",
        "# Run deduplication\n",
        "posts = deduplicate_posts(posts)\n",
        "print(f\"✅ Deduplicated to {len(posts)} unique posts\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axiTrlnAeQRL",
        "outputId": "af4a9e8d-0657-4304-90b6-539820472287"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Deduplicated to 888 unique posts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Build actor lookup ===\n",
        "\n",
        "def build_actor_lookup(posts):\n",
        "    actor_lookup = {}\n",
        "\n",
        "    for post in posts:\n",
        "        for key in [post.get(\"actor_profile\"), post.get(\"actor_backendUrn\")]:\n",
        "            if key:\n",
        "                actor_lookup[key] = {\n",
        "                    \"actor_description\": post.get(\"actor_description\", \"\"),\n",
        "                    \"actor_name\": post.get(\"actor_name\", \"\"),\n",
        "                    \"actor_profile\": post.get(\"actor_profile\", \"\"),\n",
        "                    \"actor_backendUrn\": post.get(\"actor_backendUrn\", \"\")\n",
        "                }\n",
        "    return actor_lookup\n",
        "\n",
        "# Build lookup\n",
        "actor_lookup = build_actor_lookup(posts)\n",
        "print(f\"✅ Built actor lookup with {len(actor_lookup)} actors\")\n",
        "\n",
        "# === Enrich posts with social data ===\n",
        "\n",
        "# Build social lookup\n",
        "social_lookup = {str(sd[\"entity_id\"]): sd for sd in social_data}\n",
        "\n",
        "# Existing entity ids\n",
        "existing_entity_ids = {post[\"entity_id\"] for post in posts if \"entity_id\" in post}\n",
        "\n",
        "# Helper: Create reshared post\n",
        "def create_reshared_post(post):\n",
        "    return {\n",
        "        \"entity_id\": post[\"resharedUpdate_id\"],\n",
        "        \"resharedUpdate_id\": \"\",\n",
        "        \"post_text\": post[\"post_text\"],\n",
        "        \"actor_description\": post[\"actor_description\"],\n",
        "        \"actor_name\": post[\"actor_name\"],\n",
        "        \"actor_profile\": post[\"actor_profile\"],\n",
        "        \"actor_backendUrn\": post[\"actor_backendUrn\"],\n",
        "        \"share_url\": post[\"share_url\"],\n",
        "        \"image_epoch\": post[\"image_epoch\"],\n",
        "        \"image_epoch_interpolated\": np.nan,\n",
        "        \"interpolated_time\": \"\",\n",
        "        \"numReactions\": post.get(\"numReactions\", \"\"),\n",
        "        \"numLikes\": post.get(\"numLikes\", \"\"),\n",
        "        \"numInterests\": post.get(\"numInterests\", \"\"),\n",
        "        \"numAppreciates\": post.get(\"numAppreciates\", \"\"),\n",
        "        \"numEntertains\": post.get(\"numEntertains\", \"\"),\n",
        "        \"numEmpathys\": post.get(\"numEmpathys\", \"\"),\n",
        "        \"numPraises\": post.get(\"numPraises\", \"\"),\n",
        "        \"numComments\": post.get(\"numComments\", \"\"),\n",
        "        \"numShares\": post.get(\"numShares\", \"\"),\n",
        "        \"type\": \"Original\",\n",
        "        \"sd_left\": \"\",\n",
        "        \"sd_right\": \"\"\n",
        "    }\n",
        "\n",
        "# Enrich and tag posts\n",
        "for post in posts:\n",
        "    post_id = post.get(\"entity_id\", \"\")\n",
        "    social_record = social_lookup.get(post_id)\n",
        "\n",
        "    if social_record:\n",
        "        post.update({k: v for k, v in social_record.items() if k != \"entity_id\"})\n",
        "\n",
        "    # Handle reshared logic\n",
        "    if post[\"sd_left\"] != post[\"sd_right\"] and post.get(\"type\") is None:\n",
        "        if post[\"sd_left\"] not in existing_entity_ids:\n",
        "            post[\"resharedUpdate_id\"] = post[\"sd_left\"]\n",
        "            reshared_post = create_reshared_post(post)\n",
        "            existing_entity_ids.add(reshared_post[\"entity_id\"])\n",
        "            posts.append(reshared_post)\n",
        "\n",
        "        # Update current post as Share No Comment\n",
        "        ref_id = post.get(\"header_profile_id\") or post.get(\"company_id\")\n",
        "        actor = actor_lookup.get(ref_id, {})\n",
        "\n",
        "        post.update({\n",
        "            \"post_text\": \"\",\n",
        "            \"actor_description\": actor.get(\"actor_description\", \"\"),\n",
        "            \"actor_profile\": actor.get(\"actor_profile\", \"\"),\n",
        "            \"actor_backendUrn\": actor.get(\"actor_backendUrn\", \"\"),\n",
        "            \"actor_name\": actor.get(\"actor_name\", \"\"),\n",
        "            \"share_url\": \"\",\n",
        "            \"image_epoch\": \"\",\n",
        "            \"image_epoch_interpolated\": np.nan,\n",
        "            \"interpolated_time\": \"\",\n",
        "            \"numReactions\": \"\",\n",
        "            \"numLikes\": \"\",\n",
        "            \"numInterests\": \"\",\n",
        "            \"numAppreciates\": \"\",\n",
        "            \"numEntertains\": \"\",\n",
        "            \"numEmpathys\": \"\",\n",
        "            \"numPraises\": \"\",\n",
        "            \"numComments\": \"\",\n",
        "            \"numShares\": \"\",\n",
        "            \"type\": \"Share No Comment\"\n",
        "        })\n",
        "\n",
        "    elif post[\"resharedUpdate_id\"] and not post[\"header\"] and post.get(\"type\") is None:\n",
        "        post[\"type\"] = \"Share With Comment\"\n",
        "\n",
        "    elif (not post[\"resharedUpdate_id\"]) and post.get(\"type\") is None:\n",
        "        post[\"type\"] = \"Original\"\n",
        "\n",
        "    # Clean up unused fields\n",
        "    for field in [\"socialDetail\", \"sd_left\", \"sd_right\", \"header_profile_id\", \"company_id\", \"file_segment\", \"header\"]:\n",
        "        if field in post:\n",
        "            del post[field]\n",
        "\n",
        "print(\"✅ Posts enriched and tagged\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lRg_zbdeeDe",
        "outputId": "9aa56294-242a-4c10-b0ac-fdf79f55edf6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Built actor lookup with 82 actors\n",
            "✅ Posts enriched and tagged\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Interpolate image_epoch ===\n",
        "\n",
        "def interpolate_epochs(df):\n",
        "    known_points = []\n",
        "    early_records = []\n",
        "    slope = None\n",
        "    first_known_entity_id = None\n",
        "    first_known_epoch = None\n",
        "\n",
        "    df[\"image_epoch\"] = pd.to_numeric(df[\"image_epoch\"], errors=\"coerce\")\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "\n",
        "        entity_id_numeric = int(row[\"entity_id\"])\n",
        "        current_epoch = row[\"image_epoch\"]\n",
        "\n",
        "        if pd.notna(current_epoch):\n",
        "            known_points.append((entity_id_numeric, float(current_epoch)))\n",
        "            df.at[idx, \"image_epoch_interpolated\"] = current_epoch\n",
        "\n",
        "            if len(known_points) >= 2:\n",
        "                eid_before, epoch_before = known_points[-2]\n",
        "                eid_after, epoch_after = known_points[-1]\n",
        "                slope = (epoch_after - epoch_before) / (eid_after - eid_before)\n",
        "\n",
        "                if first_known_entity_id is None:\n",
        "                    first_known_entity_id = eid_before\n",
        "                    first_known_epoch = epoch_before\n",
        "\n",
        "                for early_idx, early_eid in early_records:\n",
        "                    extrapolated_epoch = first_known_epoch - slope * (first_known_entity_id - early_eid)\n",
        "                    df.at[early_idx, \"image_epoch_interpolated\"] = int(round(extrapolated_epoch))\n",
        "                    df.at[early_idx, \"interpolated_time\"] = datetime.fromtimestamp(df.at[early_idx, \"image_epoch_interpolated\"] / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "                early_records.clear()\n",
        "\n",
        "        else:\n",
        "            if len(known_points) >= 2:\n",
        "                before = [pt for pt in known_points if pt[0] < entity_id_numeric]\n",
        "                after = [pt for pt in known_points if pt[0] > entity_id_numeric]\n",
        "\n",
        "                if before and after:\n",
        "                    eid_before, epoch_before = max(before, key=lambda x: x[0])\n",
        "                    eid_after, epoch_after = min(after, key=lambda x: x[0])\n",
        "                    local_slope = (epoch_after - epoch_before) / (eid_after - eid_before)\n",
        "                    interpolated_epoch = epoch_before + local_slope * (entity_id_numeric - eid_before)\n",
        "\n",
        "                elif before:\n",
        "                    if len(before) >= 2:\n",
        "                        b1, b2 = before[-2:]\n",
        "                        local_slope = (b2[1] - b1[1]) / (b2[0] - b1[0])\n",
        "                    else:\n",
        "                        local_slope = slope or 0\n",
        "                    eid_before, epoch_before = max(before, key=lambda x: x[0])\n",
        "                    interpolated_epoch = epoch_before + local_slope * (entity_id_numeric - eid_before)\n",
        "\n",
        "                elif after:\n",
        "                    if len(after) >= 2:\n",
        "                        a1, a2 = after[:2]\n",
        "                        local_slope = (a2[1] - a1[1]) / (a2[0] - a1[0])\n",
        "                    else:\n",
        "                        local_slope = slope or 0\n",
        "                    eid_after, epoch_after = min(after, key=lambda x: x[0])\n",
        "                    interpolated_epoch = epoch_after - local_slope * (eid_after - entity_id_numeric)\n",
        "\n",
        "                else:\n",
        "                    interpolated_epoch = 0\n",
        "\n",
        "                df.at[idx, \"image_epoch_interpolated\"] = int(round(interpolated_epoch))\n",
        "\n",
        "            else:\n",
        "                early_records.append((idx, entity_id_numeric))\n",
        "                df.at[idx, \"image_epoch_interpolated\"] = np.nan\n",
        "\n",
        "        if pd.notna(df.at[idx, \"image_epoch_interpolated\"]):\n",
        "            df.at[idx, \"interpolated_time\"] = datetime.fromtimestamp(df.at[idx, \"image_epoch_interpolated\"] / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Prepare dataframe\n",
        "df = pd.DataFrame(posts)\n",
        "df = df.sort_values(by=\"entity_id\", ascending=False)\n",
        "\n",
        "# Run interpolation\n",
        "df = interpolate_epochs(df)\n",
        "print(\"✅ Interpolation complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WtiIqdpeitC",
        "outputId": "af81c98a-cb84-4161-a566-b92a3c741113"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Interpolation complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Output CSV ===\n",
        "\n",
        "df[\"entity_id\"] = df[\"entity_id\"].astype(str)\n",
        "df[\"resharedUpdate_id\"] = df[\"resharedUpdate_id\"].astype(str)\n",
        "\n",
        "\"\"\"CSV_COLUMNS = [\n",
        "    \"entity_id\",\n",
        "    \"resharedUpdate_id\",\n",
        "    \"post_text\",\n",
        "    \"actor_name\",\n",
        "    \"actor_description\",\n",
        "    \"actor_profile\",\n",
        "    \"actor_backendUrn\",\n",
        "    \"share_url\",\n",
        "    \"numReactions\",\n",
        "    \"numLikes\",\n",
        "    \"numInterests\",\n",
        "    \"numAppreciates\",\n",
        "    \"numEntertains\",\n",
        "    \"numEmpathys\",\n",
        "    \"numPraises\",\n",
        "    \"numComments\",\n",
        "    \"numShares\",\n",
        "    \"image_epoch\",\n",
        "    \"image_epoch_interpolated\",\n",
        "    \"interpolated_time\",\n",
        "    \"type\"\n",
        "]\n",
        "\n",
        "output_file = \"/content/drive/MyDrive/LinkedInData/linkedin_posts.csv\"\n",
        "df.to_csv(output_file, index=False, encoding=\"utf-8\", columns=CSV_COLUMNS)\n",
        "\"\"\"\n",
        "print(f\"✅ CSV saved as '{output_file}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrVPzQGjenGG",
        "outputId": "7780a651-570a-47d5-b548-0cb1305f911a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CSV saved as '/content/drive/MyDrive/LinkedInData/linkedin_posts.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "#file_path = \"/content/drive/MyDrive/LinkedInData/linkedin_posts.csv\"\n",
        "#df = pd.read_csv(file_path)\n",
        "\n",
        "# Parse datetime\n",
        "df['image_datetime'] = pd.to_datetime(df['interpolated_time'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing datetime\n",
        "df = df.dropna(subset=['image_datetime'])\n",
        "\n",
        "# === FILTER FOR SELECTED ACTOR AND POST TYPES ===\n",
        "filtered_df = df.loc[\n",
        "    ((df[\"type\"] == \"Original\") | (df[\"type\"] == \"Share With Comment\") | (df[\"type\"] == \"Share No Comment\")) &\n",
        "    (df[\"actor_profile\"] == \"ACoAAAAoFkIBu5s3uIdB-WTos39dsfSNq-NNQIY\")\n",
        "]\n",
        "\n",
        "# === CORE METRICS ===\n",
        "filtered_df['date'] = filtered_df['image_datetime'].dt.date\n",
        "filtered_df['week'] = filtered_df['image_datetime'].dt.isocalendar().week\n",
        "filtered_df['year'] = filtered_df['image_datetime'].dt.year\n",
        "filtered_df['word_count'] = filtered_df['post_text'].fillna(\"\").apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# === ADDITIONAL METRICS ===\n",
        "filtered_df['day_of_week'] = filtered_df['image_datetime'].dt.day_name()\n",
        "filtered_df['char_count'] = filtered_df['post_text'].fillna(\"\").apply(len)\n",
        "filtered_df['emoji_count'] = filtered_df['post_text'].fillna(\"\").apply(lambda x: len(re.findall(r'[^\\w\\s,]', x)))\n",
        "filtered_df['hashtag_count'] = filtered_df['post_text'].fillna(\"\").apply(lambda x: len(re.findall(r'#\\w+', x)))\n",
        "\n",
        "# Calculate time gaps in hours\n",
        "filtered_df = filtered_df.sort_values('image_datetime')\n",
        "filtered_df['post_gap_seconds'] = filtered_df['image_datetime'].diff().dt.total_seconds().fillna(0)\n",
        "filtered_df['post_gap_minutes'] = filtered_df['image_datetime'].diff().dt.total_seconds().div(60).fillna(0)\n",
        "filtered_df['post_gap_hours'] = filtered_df['image_datetime'].diff().dt.total_seconds().div(3600).fillna(0)\n",
        "\n",
        "\n",
        "\n",
        "# === DAILY STATS ===\n",
        "posts_pivot = filtered_df.pivot_table(\n",
        "    index='date',\n",
        "    columns='type',\n",
        "    values='entity_id',\n",
        "    aggfunc='count',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "words_pivot = filtered_df.pivot_table(\n",
        "    index='date',\n",
        "    columns='type',\n",
        "    values='word_count',\n",
        "    aggfunc='mean',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "daily_stats = posts_pivot.merge(words_pivot, on='date', suffixes=(' Count', ' Avg Words'))\n",
        "\n",
        "# === WEEKLY STATS ===\n",
        "\"\"\"weekly_stats = filtered_df.groupby(['year', 'week']).agg(\n",
        "    posts_per_week=('post_text', 'count'),\n",
        "    avg_words_per_post=('word_count', 'mean')\n",
        ").reset_index() \"\"\"\n",
        "\n",
        "# === WEEKLY STATS ===\n",
        "weekly_posts_pivot = filtered_df.pivot_table(\n",
        "    index=['year', 'week'],\n",
        "    columns='type',\n",
        "    values='entity_id',\n",
        "    aggfunc='count',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "weekly_words_pivot = filtered_df.pivot_table(\n",
        "    index=['year', 'week'],\n",
        "    columns='type',\n",
        "    values='word_count',\n",
        "    aggfunc='mean',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "weekly_stats = weekly_posts_pivot.merge(weekly_words_pivot, on=['year', 'week'], suffixes=(' Count', ' Avg Words'))\n",
        "\n",
        "# === DAY OF WEEK STATS ===\n",
        "\"\"\"day_of_week_stats = filtered_df.groupby('day_of_week').agg(\n",
        "    total_posts=('post_text', 'count'),\n",
        "    avg_word_count=('word_count', 'mean'),\n",
        "    avg_char_count=('char_count', 'mean'),\n",
        "    avg_emoji_count=('emoji_count', 'mean'),\n",
        "    avg_hashtag_count=('hashtag_count', 'mean')\n",
        ").reset_index() \"\"\"\n",
        "\n",
        "# === DAY OF WEEK STATS ===\n",
        "day_of_week_posts_pivot = filtered_df.pivot_table(\n",
        "    index='day_of_week',\n",
        "    columns='type',\n",
        "    values='entity_id',\n",
        "    aggfunc='count',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "day_of_week_words_pivot = filtered_df.pivot_table(\n",
        "    index='day_of_week',\n",
        "    columns='type',\n",
        "    values='word_count',\n",
        "    aggfunc='mean',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "day_of_week_stats = day_of_week_posts_pivot.merge(day_of_week_words_pivot, on='day_of_week', suffixes=(' Count', ' Avg Words'))\n",
        "\n",
        "# === OVERALL METRICS ===\n",
        "\"\"\"\n",
        "overall_stats = {\n",
        "    \"total_posts\": len(filtered_df),\n",
        "    \"unique_post_days\": filtered_df['date'].nunique(),\n",
        "    \"average_posts_per_day\": filtered_df.groupby('date').size().mean(),\n",
        "    \"average_word_count\": filtered_df['word_count'].mean(),\n",
        "    \"max_word_count\": filtered_df['word_count'].max(),\n",
        "    \"min_word_count\": filtered_df['word_count'].min(),\n",
        "    \"most_active_day\": filtered_df['date'].value_counts().idxmax(),\n",
        "    \"most_active_day_count\": filtered_df['date'].value_counts().max()\n",
        "} \"\"\"\n",
        "# === OVERALL METRICS SPLIT BY TYPE ===\n",
        "overall_by_type = filtered_df.groupby('type').agg(\n",
        "    total_posts=('post_text', 'count'),\n",
        "    unique_post_days=('date', 'nunique'),\n",
        "    average_posts_per_day=('date', lambda x: x.value_counts().mean()),\n",
        "    average_word_count=('word_count', 'mean'),\n",
        "    max_word_count=('word_count', 'max'),\n",
        "    min_word_count=('word_count', 'min'),\n",
        "    most_active_day=('date', lambda x: x.value_counts().idxmax()),\n",
        "    most_active_day_count=('date', lambda x: x.value_counts().max())\n",
        ").reset_index()\n",
        "\n",
        "# === EXPORT TO MULTI-SHEET EXCEL ===\n",
        "output_path = \"/content/drive/MyDrive/LinkedInData/linkedin_combined_stats.xlsx\"\n",
        "\n",
        "with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
        "    # DAILY STATS\n",
        "    daily_stats.to_excel(writer, sheet_name='Daily_Stats', index=False)\n",
        "\n",
        "    # WEEKLY STATS\n",
        "    weekly_stats.to_excel(writer, sheet_name='Weekly_Stats', index=False)\n",
        "\n",
        "    # DAY OF WEEK STATS\n",
        "    day_of_week_stats.to_excel(writer, sheet_name='DayOfWeek_Stats', index=False)\n",
        "\n",
        "    # OVERALL METRICS\n",
        "    #overall_df = pd.DataFrame([overall_by_type])\n",
        "    #overall_df.to_excel(writer, sheet_name='Overall_Stats', index=False)\n",
        "    overall_by_type.to_excel(writer, sheet_name='Overall_Stats', index=False)\n",
        "\n",
        "    # FULL ENHANCED POST DATA\n",
        "    enhanced_columns = [\n",
        "    \"entity_id\",\n",
        "    \"resharedUpdate_id\",\n",
        "    \"post_text\",\n",
        "    \"actor_description\",\n",
        "    \"actor_name\",\n",
        "    \"actor_profile\",\n",
        "    \"actor_backendUrn\",\n",
        "    \"share_url\",\n",
        "    \"type\",\n",
        "    \"numReactions\",\n",
        "    \"numLikes\",\n",
        "    \"numInterests\",\n",
        "    \"numAppreciates\",\n",
        "    \"numEntertains\",\n",
        "    \"numEmpathys\",\n",
        "    \"numPraises\",\n",
        "    \"numComments\",\n",
        "    \"numShares\",\n",
        "    \"image_epoch\",\n",
        "    \"image_epoch_interpolated\",\n",
        "    \"interpolated_time\",\n",
        "    \"date\",\n",
        "    \"week\",\n",
        "    \"year\",\n",
        "    \"word_count\",\n",
        "    \"day_of_week\",\n",
        "    \"char_count\",\n",
        "    \"emoji_count\",\n",
        "    \"hashtag_count\",\n",
        "    \"post_gap_seconds\",\n",
        "    \"post_gap_minutes\",\n",
        "    \"post_gap_hours\"\n",
        "    ]\n",
        "\n",
        "    # Now write it in this column order:\n",
        "    filtered_df.to_excel(writer, sheet_name='Enhanced_Post_Data', index=False, columns=enhanced_columns)\n",
        "\n",
        "print(\"✅ Excel file created:\", output_path)"
      ],
      "metadata": {
        "id": "G1m-00NWxMrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "72gA7ePs4Q2M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}