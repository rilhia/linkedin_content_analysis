{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## üìÇ **Mounting Your Google Drive**\n",
        "\n",
        "Before we run the notebook, we first need to \"mount\" Google Drive. This enables your Drive files to be accessible inside Colab.  \n",
        "\n",
        "**Why do this?**\n",
        "\n",
        "üëâ You will save and read files (HAR files, outputs) directly from your Google Drive. There is no need to upload/download manually to and from Colab. Also, when using Colab anything that is kept there for the lifetime of the runtime will be lost when the runtime shuts down. Using Google Drive to handle files is so much more reliable.  \n",
        "\n",
        "### **How to do it:**\n",
        "\n",
        "1Ô∏è‚É£ Go to the code window below this section. It looks like this:  \n",
        "<img src=\"https://raw.githubusercontent.com/rilhia/linkedin_content_analysis/main/images/RunCell_1.png?raw=true\" width=\"500\">\n",
        "\n",
        "2Ô∏è‚É£ Click on the **play** ‚ñ∂Ô∏è button:  \n",
        "<img src=\"https://raw.githubusercontent.com/rilhia/linkedin_content_analysis/main/images/RunCell_2.png?raw=true\" width=\"500\">\n",
        "\n",
        "3Ô∏è‚É£ At this point, you'll be guided through the process by Google. Depending on the configuration of your system, you will see different options. Once Google Drive is mounted, you will see this:  \n",
        "<img src=\"https://raw.githubusercontent.com/rilhia/linkedin_content_analysis/main/images/MountedDrive.png?raw=true\" width=\"300\">\n",
        "\n",
        "  \n",
        "\n",
        "---\n",
        "\n",
        "Once your Drive is mounted and libraries installed, you're ready to start! How do you start?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YRgvYmq4epCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMxSu8MzeUzQ",
        "outputId": "fa31e81b-f9a9-47da-9914-6bb0a6694bd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ **Installing required libraries**\n",
        "\n",
        "Colab provides many useful libraries pre-installed ‚Äî but sometimes we need to install extra ones.\n",
        "\n",
        "In this notebook, we use **`!pip install`** to ensure we have the correct version of **`XlsxWriter`**, which we use to generate **`.xlsx`** Excel files.  \n",
        "This step is safe to run and ensures compatibility when writing out the results.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IjU-uSRGw9wU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install XlsxWriter to enable Excel file production\n",
        "!pip install XlsxWriter"
      ],
      "metadata": {
        "id": "VE6uMTp4trSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ **Imports & Utility Functions**\n",
        "\n",
        "Before we start working with the LinkedIn HAR data, we need to load some essential **Python libraries** and define a few **helper functions**.\n",
        "\n",
        "### üõ†Ô∏è **What this section does:**\n",
        "\n",
        "‚úÖ Loads core libraries:\n",
        "\n",
        "- `json` ‚Äî to parse the HAR file (which is in JSON format)  \n",
        "- `pandas` ‚Äî for powerful data handling and analysis  \n",
        "- `re` ‚Äî for regular expressions (pattern matching, used to extract IDs, hashtags, etc.)  \n",
        "- `numpy` ‚Äî for numerical operations (used in interpolations)  \n",
        "- `copy` ‚Äî to safely duplicate data structures  \n",
        "- `glob` ‚Äî to easily load multiple HAR files  \n",
        "- `datetime` ‚Äî for converting and formatting timestamps\n",
        "---\n",
        "\n",
        "\n",
        "‚úÖ Defines **utility functions**:\n",
        "\n",
        "- 1Ô∏è‚É£ `safe_get()`  \n",
        "Safely navigate a deeply nested dictionary without errors if something is missing. Very useful when dealing with LinkedIn‚Äôs complex JSON.\n",
        "\n",
        "- 2Ô∏è‚É£ `extract_file_segment()`  \n",
        "Extracts a unique part of the LinkedIn image URL ‚Äî this contains an **epoch timestamp** (which we will later use to figure out when posts were published).\n",
        "\n",
        "- 3Ô∏è‚É£ `extract_epoch_from_segment()`  \n",
        "Takes the image URL segment and extracts the actual **epoch time** (milliseconds since 1970) so we can convert it to a readable date/time.\n",
        "\n",
        "- 4Ô∏è‚É£ `extract_entity_id()`  \n",
        "Extracts **LinkedIn post or activity IDs** from special LinkedIn URN strings (e.g. `\"urn:li:activity:1234567890123\"`).\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Why do this first?**\n",
        "\n",
        "The raw HAR files from LinkedIn are:\n",
        "\n",
        "- **Very large**\n",
        "- **Highly nested**\n",
        "- **Messy**\n",
        "\n",
        "These utilities give us **clean, reusable ways** to pull out exactly what we need (timestamps, post IDs, text content, etc.) so that the rest of the notebook can work smoothly.\n",
        "\n",
        "---\n",
        "\n",
        "üëâ After this section, we will load the HAR files and start extracting the **actual post data**!\n",
        "\n",
        "But first, we press **play** ‚ñ∂Ô∏è to run this code..."
      ],
      "metadata": {
        "id": "R4inThw95ICx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPvN3c9Dd33T"
      },
      "outputs": [],
      "source": [
        "# === Imports ===\n",
        "\n",
        "# Core modules\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import copy\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "# === Utility Functions ===\n",
        "\n",
        "def safe_get(data, path, default=\"\"):\n",
        "    \"\"\"\n",
        "    Safely navigate a nested dictionary using a list of keys.\n",
        "\n",
        "    Args:\n",
        "        data (dict): The dictionary to search.\n",
        "        path (list): A list of keys representing the path.\n",
        "        default (any): Default value to return if the path doesn't exist.\n",
        "\n",
        "    Returns:\n",
        "        The value found at the specified path or the default value.\n",
        "    \"\"\"\n",
        "    for key in path:\n",
        "        if isinstance(data, dict) and key in data:\n",
        "            data = data[key]\n",
        "        else:\n",
        "            return default\n",
        "    return default if data is None else data\n",
        "\n",
        "\n",
        "def extract_file_segment(item):\n",
        "    \"\"\"\n",
        "    Extract the fileIdentifyingUrlPathSegment from an image artifact in the item.\n",
        "    This is used to retrieve an embedded timestamp (epoch) from the image URL.\n",
        "\n",
        "    Args:\n",
        "        item (dict): A dictionary representing a LinkedIn post item.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted file segment, or empty string if not found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        images = item.get(\"content\", {}).get(\"imageComponent\", {}).get(\"images\", [])\n",
        "        for img in images:\n",
        "            for attr in img.get(\"attributes\", []):\n",
        "                vector = attr.get(\"detailData\", {}).get(\"vectorImage\", {})\n",
        "                for artifact in vector.get(\"artifacts\", []):\n",
        "                    if \"fileIdentifyingUrlPathSegment\" in artifact:\n",
        "                        return artifact[\"fileIdentifyingUrlPathSegment\"]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def extract_epoch_from_segment(segment):\n",
        "    \"\"\"\n",
        "    Extract the epoch timestamp from an image fileIdentifyingUrlPathSegment.\n",
        "    Example segment format: \"/path/to/image/1716789123456?params\"\n",
        "\n",
        "    Args:\n",
        "        segment (str): The URL path segment.\n",
        "\n",
        "    Returns:\n",
        "        int or None: The extracted epoch timestamp, or None if not valid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        parts = segment.split(\"/\")\n",
        "        if len(parts) >= 4:\n",
        "            timestamp_str = parts[3].split(\"?\")[0]\n",
        "            if timestamp_str.isdigit():\n",
        "                return int(timestamp_str)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "\n",
        "def extract_entity_id(urn_str):\n",
        "    \"\"\"\n",
        "    Extract the numeric entity ID from a LinkedIn URN string.\n",
        "    Example URN: \"urn:li:activity:1234567890123\"\n",
        "\n",
        "    Args:\n",
        "        urn_str (str): The URN string.\n",
        "\n",
        "    Returns:\n",
        "        int or None: The numeric ID, or None if not found.\n",
        "    \"\"\"\n",
        "    match = re.search(r'\\d{6,}', urn_str or \"\")\n",
        "    return int(match.group()) if match else None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÇ **Loading HAR Files (Raw LinkedIn Data)**\n",
        "\n",
        "Now that we have our **helper functions** ready, it's time to load the actual data!\n",
        "\n",
        "### üóÇÔ∏è **What is a HAR file?**\n",
        "\n",
        "- HAR stands for **HTTP Archive**. It‚Äôs a special file format that records the network traffic between your browser (Chrome) and a website (LinkedIn in this case).\n",
        "- When you scroll through LinkedIn posts with **Developer Tools open**, the browser is fetching the **raw post data** and other details ‚Äî this gets recorded in the HAR file.\n",
        "\n",
        "### üöÄ **What this section does:**\n",
        "\n",
        "‚úÖ Optionally shows **how to load a single HAR file** (example is commented out ‚Äî useful if you want to test one file and have several available in your storage location).\n",
        "\n",
        "<img src=\"https://github.com/rilhia/linkedin_content_analysis/blob/main/images/SingleHARFile.png?raw=true\" width=\"500\">\n",
        "\n",
        "The green box shows the comment quotes **`\"\"\"`** being commented out using a hash **`#`**.\n",
        "\n",
        "The red box shows the comment quotes being used to comment out the **`ALL HAR files`** section.\n",
        "\n",
        "1. You define the filename here:\n",
        "    ```python\n",
        "    filename = \"/content/drive/MyDrive/LinkedInData/www.linkedin.com.har\"\n",
        "    ```\n",
        "\n",
        "\n",
        "‚úÖ Automatically **loads ALL HAR files** from your selected folder:\n",
        "\n",
        "<img src=\"https://github.com/rilhia/linkedin_content_analysis/blob/main/images/MultipleHARFile.png?raw=true\" width=\"500\">\n",
        "\n",
        "The red box shows the comment quotes being used to comment out the **`OPTIONAL`** section.\n",
        "\n",
        "The green box shows the comment quotes **`\"\"\"`** being commented out using a hash **`#`**.\n",
        "\n",
        "1. You define the folder here:\n",
        "    ```python\n",
        "    har_folder = \"/content/drive/MyDrive/LinkedInData\"\n",
        "    ```\n",
        "\n",
        "2. It finds all `.har` files in that folder:\n",
        "    ```python\n",
        "    har_files = glob.glob(f\"{har_folder}/*.har\")\n",
        "    ```\n",
        "\n",
        "3. It loops through each HAR file and extracts the key part:\n",
        "    ```python\n",
        "    entries = har_data.get(\"log\", {}).get(\"entries\", [])\n",
        "    ```\n",
        "\n",
        "4. It combines **all entries** into one big list: `all_entries`\n",
        "\n",
        "‚úÖ Prepares a **combined HAR data structure** (`combined_har_data`) so it looks like a full HAR file ‚Äî this allows our later functions (like the post extractor) to run on **all files at once**.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Why do we merge multiple HAR files?**\n",
        "\n",
        "- When browsing LinkedIn, your HAR file will only capture the posts you scroll past during that session.\n",
        "- If you're researching a creator with **hundreds of posts**, you‚Äôll probably want to do **multiple scroll & save passes** (and save multiple HAR files).\n",
        "- This section lets you merge them automatically into one combined dataset.\n",
        "\n",
        "---\n",
        "\n",
        "üëâ Next, we‚Äôll run the **post extraction** to pull out the actual post content and engagement data!"
      ],
      "metadata": {
        "id": "PXqzCITW7NMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load HAR Files ===\n",
        "\n",
        "# Initialize an empty list to hold all log entries\n",
        "all_entries = []\n",
        "\n",
        "# OPTIONAL: Single HAR file example (commented out)\n",
        "#\"\"\"\n",
        "# Example of loading a single HAR file\n",
        "filename = \"/content/drive/MyDrive/LinkedInData/www.linkedin.com.har\"\n",
        "\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    har_data = json.load(f)\n",
        "\n",
        "    # Extract the 'entries' from this HAR file\n",
        "    entries = har_data.get(\"log\", {}).get(\"entries\", [])\n",
        "\n",
        "    # Add these entries to our master list\n",
        "    all_entries.extend(entries)\n",
        "#\"\"\"\n",
        "\n",
        "\n",
        "# === Load ALL HAR files from folder ===\n",
        "\"\"\"\n",
        "\n",
        "# Define the folder where HAR files are stored\n",
        "har_folder = \"/content/drive/MyDrive/LinkedInData\"\n",
        "\n",
        "# Find all .har files in the folder\n",
        "har_files = glob.glob(f\"{har_folder}/*.har\")\n",
        "\n",
        "# Print how many HAR files were found\n",
        "print(f\"‚úÖ Found {len(har_files)} HAR files\")\n",
        "\n",
        "# Loop through each HAR file and load its contents\n",
        "for filename in har_files:\n",
        "    print(f\"Loading: {filename}\")\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        har_data = json.load(f)\n",
        "\n",
        "        # Extract the 'entries' from this HAR file\n",
        "        entries = har_data.get(\"log\", {}).get(\"entries\", [])\n",
        "\n",
        "        # Add these entries to our master list\n",
        "        all_entries.extend(entries)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Final print: total number of entries combined from all files\n",
        "print(f\"‚úÖ Total entries loaded: {len(all_entries)}\")\n",
        "\n",
        "# === Prepare combined HAR data structure ===\n",
        "# Some functions expect HAR data in full \"har_data\" format.\n",
        "# We can fake a combined one using our all_entries list:\n",
        "combined_har_data = {\"log\": {\"entries\": all_entries}}"
      ],
      "metadata": {
        "id": "wuUJxJKkeDkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù **Extracting Posts and Social Engagement Data**\n",
        "\n",
        "Now that we‚Äôve **loaded all our HAR files** into memory, it‚Äôs time to extract the **useful content**:\n",
        "\n",
        "‚úÖ Actual **LinkedIn posts**  \n",
        "‚úÖ **Engagement metrics** (likes, comments, shares, reactions)\n",
        "\n",
        "---\n",
        "\n",
        "### **What this section does:**\n",
        "\n",
        "We define a function:\n",
        "\n",
        "```python\n",
        "extract_posts_and_social(har_data)\n",
        "```\n",
        "\n",
        "It takes our combined HAR data and pulls out:\n",
        "\n",
        "1Ô∏è‚É£ Post Data ‚Üí Stored in posts list  \n",
        "2Ô∏è‚É£ Social Data ‚Üí Stored in social_data list  \n",
        "\n",
        "---\n",
        "\n",
        "### **How does it work?**\n",
        "\n",
        "1Ô∏è‚É£ It loops through each HAR ‚Äúentry‚Äù (which is an HTTP request/response):  \n",
        "\n",
        "```python\n",
        "for entry in har_data.get(\"log\", {}).get(\"entries\", []):\n",
        "```\n",
        "\n",
        "2Ô∏è‚É£ It filters only LinkedIn GraphQL requests:  \n",
        "\n",
        "These requests contain the actual post data in the HAR:  \n",
        "\n",
        "```python\n",
        "if not url.startswith(\"https://www.linkedin.com/voyager/api/graphql?\"):\n",
        "    continue\n",
        "```\n",
        "\n",
        "3Ô∏è‚É£ For each GraphQL response:  \n",
        "\n",
        "It looks inside:  \n",
        "\n",
        "```python\n",
        "response_json.get(\"included\", [])\n",
        "```\n",
        "\n",
        "4Ô∏è‚É£ It extracts two types of content:  \n",
        "\n",
        "**A) Posts**  \n",
        "\t‚Ä¢\tIf the item is:  \n",
        "  ```python\n",
        "  \"$type\" == \"com.linkedin.voyager.dash.feed.SocialActivityCounts\"\n",
        "  ```\n",
        "\n",
        "We extract:  \n",
        "\t‚Ä¢\tPost text  \n",
        "\t‚Ä¢\tAuthor name & profile  \n",
        "\t‚Ä¢\tPost ID  \n",
        "\t‚Ä¢\tIf it‚Äôs a reshare (and link to original post)  \n",
        "\t‚Ä¢\tEmbedded image timestamp (used to infer post time!)  \n",
        "\t‚Ä¢\tShare URL  \n",
        "\t‚Ä¢\tVarious IDs we‚Äôll need for linking posts together later  \n",
        "\n",
        "**B) Social Data**.  \n",
        "\t‚Ä¢\tIf the item is:  \n",
        "  ```python\n",
        "  \"$type\" == \"com.linkedin.voyager.dash.feed.SocialActivityCounts\"\n",
        "  ```\n",
        "\n",
        "We extract:  \n",
        "\t‚Ä¢\tTotal reactions  \n",
        "\t‚Ä¢\tLikes  \n",
        "\t‚Ä¢\tComments  \n",
        "\t‚Ä¢\tShares  \n",
        "\t‚Ä¢\tDetailed reaction types (Appreciation, Empathy, Entertainment, etc.)  \n",
        "\n",
        "---\n",
        "### **Why do we separate posts and social data?**\n",
        "\n",
        "Because LinkedIn returns them in different API items:  \n",
        "\t‚Ä¢\tThe ‚ÄúUpdate‚Äù object contains the post itself  \n",
        "\t‚Ä¢\tThe ‚ÄúSocialActivityCounts‚Äù object contains the engagement metrics\n",
        "\n",
        "---\n",
        "### **What happens at the end?**\n",
        "\n",
        "We run the extraction:  \n",
        "\n",
        "```python\n",
        "posts, social_data = extract_posts_and_social(combined_har_data)\n",
        "```\n",
        "\n",
        "And we print a summary:  \n",
        "```\n",
        "‚úÖ Extracted X posts and Y social records\n",
        "```\n",
        "---\n",
        "### **Summary**\n",
        "\n",
        "At this point, we now have two key lists ready for further processing:  \n",
        "\n",
        "üìÑ posts ‚Üí All posts with text, author, timestamps, resharing info  \n",
        "üìä social_data ‚Üí All reactions, likes, comments, shares  \n",
        "\n",
        "---\n",
        "\n",
        "üëâ In the next step, we‚Äôll deduplicate posts (LinkedIn sometimes sends duplicate data in HAR files).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qg1LW3XIAQCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Extract posts & social data ===\n",
        "\n",
        "def extract_posts_and_social(har_data):\n",
        "    \"\"\"\n",
        "    Extracts:\n",
        "    - Post content and metadata from LinkedIn HAR data\n",
        "    - Social engagement counts (likes, comments, shares, etc.)\n",
        "\n",
        "    Returns:\n",
        "    - posts: List of dictionaries (one per post)\n",
        "    - social_data: List of dictionaries (one per social engagement record)\n",
        "    \"\"\"\n",
        "\n",
        "    posts = []\n",
        "    social_data = []\n",
        "\n",
        "    # Loop over all HAR entries (HTTP requests/responses)\n",
        "    for entry in har_data.get(\"log\", {}).get(\"entries\", []):\n",
        "\n",
        "        # Only process GraphQL requests (where LinkedIn post data lives)\n",
        "        url = entry.get(\"request\", {}).get(\"url\", \"\")\n",
        "        if not url.startswith(\"https://www.linkedin.com/voyager/api/graphql?\"):\n",
        "            continue\n",
        "\n",
        "        # Try to decode JSON response\n",
        "        text_data = entry.get(\"response\", {}).get(\"content\", {}).get(\"text\", \"\")\n",
        "        try:\n",
        "            response_json = json.loads(text_data)\n",
        "        except json.JSONDecodeError:\n",
        "            continue  # Skip invalid JSON\n",
        "\n",
        "        # Loop through all \"included\" items in this GraphQL response\n",
        "        for item in response_json.get(\"included\", []):\n",
        "\n",
        "            # === If this is a Post ===\n",
        "            if item.get(\"$type\") == \"com.linkedin.voyager.dash.feed.Update\":\n",
        "\n",
        "                # Extract profile info (actor)\n",
        "                attributes = safe_get(item, [\"actor\", \"name\", \"attributesV2\"], [])\n",
        "                actor_profile = safe_get(attributes[0], [\"detailData\", \"*profileFullName\"]) if attributes else \"\"\n",
        "                actor_profile_id = actor_profile.split(\"profile:\")[1] if \"profile:\" in actor_profile else \"\"\n",
        "\n",
        "                # Extract image timestamp (used for post time)\n",
        "                file_segment = extract_file_segment(item)\n",
        "                image_epoch = extract_epoch_from_segment(file_segment)\n",
        "                image_datetime = datetime.fromtimestamp(image_epoch / 1000).strftime('%Y-%m-%d %H:%M:%S') if image_epoch else \"\"\n",
        "\n",
        "                # Extract post metadata\n",
        "                entity_urn = safe_get(item, [\"metadata\", \"backendUrn\"])\n",
        "                header = safe_get(item, [\"header\", \"text\", \"text\"])\n",
        "\n",
        "                # Extract reshared post (if any)\n",
        "                resharedUpdate = safe_get(item, [\"*resharedUpdate\"])\n",
        "                resharedUpdate_id = extract_entity_id(resharedUpdate)\n",
        "                entity_id = extract_entity_id(entity_urn)\n",
        "\n",
        "                # Extract additional actor info (header profile, company, etc.)\n",
        "                header_attributes = safe_get(item, [\"header\", \"text\", \"attributesV2\"], [])\n",
        "                header_profile = safe_get(header_attributes[0], [\"detailData\", \"*profileFullName\"]) if header_attributes else \"\"\n",
        "                header_profile_id = header_profile.split(\"profile:\")[1] if \"profile:\" in header_profile else \"\"\n",
        "\n",
        "                commentary_attributes = safe_get(item, [\"commentary\", \"text\", \"attributesV2\"], [])\n",
        "                company_id = safe_get(commentary_attributes[0], [\"detailData\", \"*companyName\"]) if commentary_attributes else None\n",
        "\n",
        "                # Extract social detail links (used for linking shared posts)\n",
        "                socialDetail = safe_get(item, [\"*socialDetail\"])\n",
        "                matches = re.findall(r'urn:li:(?:activity|ugcPost):(\\d+)\\b|urn:li:groupPost:\\d+-(\\d+)\\b', socialDetail)\n",
        "                flattened_matches = [m[0] or m[1] for m in matches if m[0] or m[1]]\n",
        "                sd_left = flattened_matches[0] if len(matches) >= 2 else None\n",
        "                sd_right = flattened_matches[1] if len(matches) >= 2 else None\n",
        "\n",
        "                # Build post dictionary\n",
        "                post_data = {\n",
        "                    \"entity_id\": str(entity_id) if entity_id else \"\",\n",
        "                    \"resharedUpdate_id\": str(resharedUpdate_id) if resharedUpdate_id else \"\",\n",
        "                    \"header\": header,\n",
        "                    \"post_text\": safe_get(item, [\"commentary\", \"text\", \"text\"]),\n",
        "                    \"actor_description\": safe_get(item, [\"actor\", \"description\", \"text\"]),\n",
        "                    \"actor_name\": safe_get(item, [\"actor\", \"name\", \"text\"]),\n",
        "                    \"actor_profile\": actor_profile_id,\n",
        "                    \"actor_backendUrn\": safe_get(item, [\"actor\", \"backendUrn\"]),\n",
        "                    \"share_url\": safe_get(item, [\"socialContent\", \"shareUrl\"]),\n",
        "                    \"file_segment\": file_segment,\n",
        "                    \"image_epoch\": image_epoch,\n",
        "                    \"image_datetime\": image_datetime,\n",
        "                    \"socialDetail\": socialDetail,\n",
        "                    \"sd_left\": sd_left,\n",
        "                    \"sd_right\": sd_right,\n",
        "                    \"header_profile_id\": header_profile_id,\n",
        "                    \"company_id\": company_id\n",
        "                }\n",
        "\n",
        "                # Add post to list\n",
        "                posts.append(post_data)\n",
        "\n",
        "            # === If this is Social Data (reactions, comments, shares) ===\n",
        "            elif item.get(\"$type\") == \"com.linkedin.voyager.dash.feed.SocialActivityCounts\":\n",
        "\n",
        "                # Extract social data\n",
        "                entity_urn = safe_get(item, [\"entityUrn\"])\n",
        "                entity_id = extract_entity_id(entity_urn)\n",
        "\n",
        "                # Raw counts\n",
        "                numReactions = safe_get(item, [\"numLikes\"])\n",
        "                numComments = safe_get(item, [\"numComments\"])\n",
        "                numShares = safe_get(item, [\"numShares\"])\n",
        "\n",
        "                # Detailed reaction types (like, interest, appreciation, etc.)\n",
        "                reactionTypeCounts = safe_get(item, [\"reactionTypeCounts\"], [])\n",
        "                reaction_map = {r.get(\"reactionType\"): r.get(\"count\", 0) for r in reactionTypeCounts}\n",
        "\n",
        "                # Build social data dictionary\n",
        "                sd = {\n",
        "                    \"entity_id\": entity_id,\n",
        "                    \"numReactions\": numReactions,\n",
        "                    \"numLikes\": reaction_map.get(\"LIKE\", 0),\n",
        "                    \"numInterests\": reaction_map.get(\"INTEREST\", 0),\n",
        "                    \"numAppreciates\": reaction_map.get(\"APPRECIATION\", 0),\n",
        "                    \"numEntertains\": reaction_map.get(\"ENTERTAINMENT\", 0),\n",
        "                    \"numEmpathys\": reaction_map.get(\"EMPATHY\", 0),\n",
        "                    \"numPraises\": reaction_map.get(\"PRAISE\", 0),\n",
        "                    \"numComments\": numComments,\n",
        "                    \"numShares\": numShares\n",
        "                }\n",
        "\n",
        "                # Add social data to list\n",
        "                social_data.append(sd)\n",
        "\n",
        "    # Done ‚Äî return both lists\n",
        "    return posts, social_data\n",
        "\n",
        "\n",
        "# === Run Extraction ===\n",
        "\n",
        "# Run the extraction function on combined HAR data\n",
        "posts, social_data = extract_posts_and_social(combined_har_data)\n",
        "\n",
        "# Print summary\n",
        "print(f\"‚úÖ Extracted {len(posts)} posts and {len(social_data)} social records from ALL files\")"
      ],
      "metadata": {
        "id": "FxmZKtB0eJHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **What this step does**\n",
        "\n",
        "We define a function:\n",
        "\n",
        "`deduplicate_posts(posts)`\n",
        "\n",
        "Its job is to:\n",
        "\n",
        "- **Keep only ONE post** per unique `entity_id`\n",
        "- If duplicates are found, prefer the version with **more complete data** (non-empty fields)\n",
        "\n",
        "---\n",
        "\n",
        "### **How it works**\n",
        "\n",
        "1Ô∏è‚É£ We build a dictionary:\n",
        "\n",
        "    best_posts = {}\n",
        "\n",
        "- **Key** = `entity_id`\n",
        "- **Value** = \"best\" version of the post for that ID\n",
        "\n",
        "---\n",
        "\n",
        "2Ô∏è‚É£ For each post:\n",
        "\n",
        "- If it‚Äôs the **first time** seeing that `entity_id`, we store it\n",
        "- If we already have a version:\n",
        "    - We check each field\n",
        "    - If the new version has **better data** (not `None` / not empty), we update the field\n",
        "\n",
        "---\n",
        "\n",
        "### **Why is this safe?**\n",
        "\n",
        "- `entity_id` is a **LinkedIn post ID**. You only get one `entity_id` per post\n",
        "- We know that any true duplicate posts will share this ID\n",
        "\n",
        "---\n",
        "\n",
        "### **Result**\n",
        "\n",
        "After running:\n",
        "\n",
        "    posts = deduplicate_posts(posts)\n",
        "\n",
        "You will have a **clean list of unique posts** ‚Äî one per LinkedIn post.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary printed**\n",
        "\n",
        "Example output:\n",
        "\n",
        "```\n",
        "‚úÖ Deduplicated to X unique posts\n",
        "```\n",
        "---\n",
        "\n",
        "Next, we‚Äôll start **building lookups** and enriching the posts with engagement data (likes, comments, shares).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ZpyuHlsJGncC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Deduplicate posts ===\n",
        "\n",
        "def deduplicate_posts(posts):\n",
        "    \"\"\"\n",
        "    Deduplicates posts based on 'entity_id'.\n",
        "\n",
        "    Why?\n",
        "    Sometimes multiple HAR files may capture the same post multiple times.\n",
        "    We want ONE clean version of each post.\n",
        "\n",
        "    Strategy:\n",
        "    - Keep only ONE post per unique entity_id.\n",
        "    - If we find duplicates, we prefer the post version that contains more complete data.\n",
        "\n",
        "    Input:\n",
        "    - posts: List of post dictionaries\n",
        "\n",
        "    Returns:\n",
        "    - List of deduplicated posts\n",
        "    \"\"\"\n",
        "\n",
        "    best_posts = {}  # Dictionary to hold one \"best\" post per entity_id\n",
        "\n",
        "    for post in posts:\n",
        "        post_id = str(post.get(\"entity_id\", \"\"))\n",
        "        if not post_id:\n",
        "            continue  # Skip posts with no ID (shouldn't happen, but safe)\n",
        "\n",
        "        if post_id not in best_posts:\n",
        "            # First time seeing this post ‚Äî store it\n",
        "            best_posts[post_id] = copy.deepcopy(post)\n",
        "        else:\n",
        "            # Already have a version of this post ‚Äî check if this one has better data\n",
        "            current_best = best_posts[post_id]\n",
        "\n",
        "            # Go through each field\n",
        "            for key, value in post.items():\n",
        "                if key not in current_best:\n",
        "                    current_best[key] = value\n",
        "                elif current_best[key] in [None, \"\", [], {}] and value not in [None, \"\", [], {}]:\n",
        "                    # If the current value is empty but this one has data ‚Äî replace it\n",
        "                    current_best[key] = value\n",
        "\n",
        "    # Return the list of deduplicated posts\n",
        "    return list(best_posts.values())\n",
        "\n",
        "\n",
        "# === Run deduplication ===\n",
        "\n",
        "posts = deduplicate_posts(posts)\n",
        "\n",
        "# Summary print\n",
        "print(f\"‚úÖ Deduplicated to {len(posts)} unique posts\")"
      ],
      "metadata": {
        "id": "axiTrlnAeQRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üîç **Building Actor Lookup + Enriching Posts with Social Data**\n",
        "\n",
        "---\n",
        "\n",
        "### **What this step does:**\n",
        "\n",
        "Now that we have extracted **post data** and **social engagement data** from the HAR files, we need to prepare the data for deeper analysis.\n",
        "\n",
        "This section does two main things:\n",
        "\n",
        "---\n",
        "\n",
        "### 1Ô∏è‚É£ **Build an \"Actor Lookup\"**\n",
        "\n",
        "- Many posts refer to *other users* (such as in **shares**, **comments**, or **reshared posts**).\n",
        "- To avoid duplication and simplify enrichment, we build a simple **lookup dictionary**:\n",
        "    - Keys: `actor_profile` (LinkedIn profile ID) or `actor_backendUrn`\n",
        "    - Values: Actor details (name, description, profile)\n",
        "\n",
        "This lets us easily **fill in missing info** for reshared posts later.\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ **Enrich Posts with Social Data + Classify Post Type**\n",
        "\n",
        "We now loop through all extracted posts:\n",
        "\n",
        "‚úÖ **Enrich each post with social metrics** (likes, comments, shares, reactions).\n",
        "\n",
        "‚úÖ **Detect and handle reshared posts:**\n",
        "\n",
        "- **Case 1:** *Share with no comment* ‚Üí just a pure share ‚Üí we label it `\"Share No Comment\"`.\n",
        "    - If we don't already have the original post, we create a placeholder for it.\n",
        "- **Case 2:** *Share with comment* ‚Üí user adds their own commentary ‚Üí we label it `\"Share With Comment\"`.\n",
        "- **Case 3:** *Original post* ‚Üí standalone post ‚Üí we label it `\"Original\"`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why is this needed?**\n",
        "\n",
        "üëâ Many LinkedIn \"shares\" and \"reshared posts\" link **across posts**.\n",
        "\n",
        "üëâ The HAR data **does not always include the full original post** ‚Äî so we must **track these links** and ensure we create a **clean, unified dataset** where:\n",
        "\n",
        "- Post types are clear.\n",
        "- Social metrics are attached.\n",
        "- Links between shares and originals are preserved.\n",
        "- Actor details are consistently populated.\n",
        "\n",
        "---\n",
        "\n",
        "### **Result:**\n",
        "\n",
        "At the end of this step, we will have:\n",
        "\n",
        "‚úÖ All posts tagged as:\n",
        "- `\"Original\"`\n",
        "- `\"Share With Comment\"`\n",
        "- `\"Share No Comment\"`\n",
        "\n",
        "‚úÖ All posts enriched with:\n",
        "- Likes\n",
        "- Comments\n",
        "- Shares\n",
        "- Reaction types\n",
        "\n",
        "‚úÖ Links between reshared posts and original posts clearly mapped.\n",
        "\n",
        "‚úÖ Cleaned fields ‚Äî only the relevant data kept.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary:**\n",
        "\n",
        "This is a **crucial cleaning step** before moving on to:  \n",
        "‚û°Ô∏è **time-based analysis**  \n",
        "‚û°Ô∏è **engagement trends**  \n",
        "‚û°Ô∏è **network structure analysis** (who shares whose content).\n",
        "\n",
        "---\n",
        "\n",
        "üöÄ Final print:\n",
        "```\n",
        "‚úÖ Posts enriched and tagged.\n",
        "```\n"
      ],
      "metadata": {
        "id": "6aUAbgP8IiaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Build actor lookup ===\n",
        "\n",
        "def build_actor_lookup(posts):\n",
        "    \"\"\"\n",
        "    Creates a lookup dictionary of actors (post authors).\n",
        "\n",
        "    Why?\n",
        "    Some posts reference other users (such as in shares), and we want to\n",
        "    easily look up their profile info (name, description, etc).\n",
        "\n",
        "    The lookup uses:\n",
        "    - actor_profile (profile ID string)\n",
        "    - actor_backendUrn (LinkedIn backend URN)\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary: { actor_key : { actor details } }\n",
        "    \"\"\"\n",
        "    actor_lookup = {}\n",
        "\n",
        "    for post in posts:\n",
        "        # Use both profile ID and backend URN as possible keys\n",
        "        for key in [post.get(\"actor_profile\"), post.get(\"actor_backendUrn\")]:\n",
        "            if key:\n",
        "                actor_lookup[key] = {\n",
        "                    \"actor_description\": post.get(\"actor_description\", \"\"),\n",
        "                    \"actor_name\": post.get(\"actor_name\", \"\"),\n",
        "                    \"actor_profile\": post.get(\"actor_profile\", \"\"),\n",
        "                    \"actor_backendUrn\": post.get(\"actor_backendUrn\", \"\")\n",
        "                }\n",
        "\n",
        "    return actor_lookup\n",
        "\n",
        "\n",
        "# Build the lookup\n",
        "actor_lookup = build_actor_lookup(posts)\n",
        "print(f\"‚úÖ Built actor lookup with {len(actor_lookup)} actors\")\n",
        "\n",
        "\n",
        "# === Enrich posts with social data ===\n",
        "\n",
        "# Step 1: Build social lookup\n",
        "# This makes it fast to look up social metrics for each post\n",
        "social_lookup = {str(sd[\"entity_id\"]): sd for sd in social_data}\n",
        "\n",
        "# Step 2: Track which entity_ids already exist (for deduplication when adding reshared posts)\n",
        "existing_entity_ids = {post[\"entity_id\"] for post in posts if \"entity_id\" in post}\n",
        "\n",
        "\n",
        "# === Helper function to create a reshared post ===\n",
        "\n",
        "def create_reshared_post(post):\n",
        "    \"\"\"\n",
        "    Creates a minimal 'reshared' post record based on the original post.\n",
        "    Useful when we detect a post was reshared but we have no original record yet.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"entity_id\": post[\"resharedUpdate_id\"],\n",
        "        \"resharedUpdate_id\": \"\",\n",
        "        \"post_text\": post[\"post_text\"],\n",
        "        \"actor_description\": post[\"actor_description\"],\n",
        "        \"actor_name\": post[\"actor_name\"],\n",
        "        \"actor_profile\": post[\"actor_profile\"],\n",
        "        \"actor_backendUrn\": post[\"actor_backendUrn\"],\n",
        "        \"share_url\": post[\"share_url\"],\n",
        "        \"image_epoch\": post[\"image_epoch\"],\n",
        "        \"image_epoch_interpolated\": np.nan,\n",
        "        \"interpolated_time\": \"\",\n",
        "        \"numReactions\": post.get(\"numReactions\", \"\"),\n",
        "        \"numLikes\": post.get(\"numLikes\", \"\"),\n",
        "        \"numInterests\": post.get(\"numInterests\", \"\"),\n",
        "        \"numAppreciates\": post.get(\"numAppreciates\", \"\"),\n",
        "        \"numEntertains\": post.get(\"numEntertains\", \"\"),\n",
        "        \"numEmpathys\": post.get(\"numEmpathys\", \"\"),\n",
        "        \"numPraises\": post.get(\"numPraises\", \"\"),\n",
        "        \"numComments\": post.get(\"numComments\", \"\"),\n",
        "        \"numShares\": post.get(\"numShares\", \"\"),\n",
        "        \"type\": \"Original\",  # Mark as original because it‚Äôs the original content\n",
        "        \"sd_left\": \"\",\n",
        "        \"sd_right\": \"\"\n",
        "    }\n",
        "\n",
        "\n",
        "# === Main loop: Enrich and tag posts ===\n",
        "\n",
        "for post in posts:\n",
        "    post_id = post.get(\"entity_id\", \"\")\n",
        "\n",
        "    # Add social metrics to this post if available\n",
        "    social_record = social_lookup.get(post_id)\n",
        "    if social_record:\n",
        "        post.update({k: v for k, v in social_record.items() if k != \"entity_id\"})\n",
        "\n",
        "    # === Handling reshared posts ===\n",
        "\n",
        "    # Case 1: Reshare with NO COMMENT (just a pure share of another post)\n",
        "    if post[\"sd_left\"] != post[\"sd_right\"] and post.get(\"type\") is None:\n",
        "        # If we don't already have this reshared post in our dataset, create it\n",
        "        if post[\"sd_left\"] not in existing_entity_ids:\n",
        "            post[\"resharedUpdate_id\"] = post[\"sd_left\"]\n",
        "            reshared_post = create_reshared_post(post)\n",
        "            existing_entity_ids.add(reshared_post[\"entity_id\"])\n",
        "            posts.append(reshared_post)\n",
        "\n",
        "        # Now update THIS post to mark it as \"Share No Comment\"\n",
        "        ref_id = post.get(\"header_profile_id\") or post.get(\"company_id\")\n",
        "        actor = actor_lookup.get(ref_id, {})\n",
        "\n",
        "        post.update({\n",
        "            \"post_text\": \"\",\n",
        "            \"actor_description\": actor.get(\"actor_description\", \"\"),\n",
        "            \"actor_profile\": actor.get(\"actor_profile\", \"\"),\n",
        "            \"actor_backendUrn\": actor.get(\"actor_backendUrn\", \"\"),\n",
        "            \"actor_name\": actor.get(\"actor_name\", \"\"),\n",
        "            \"share_url\": \"\",\n",
        "            \"image_epoch\": \"\",\n",
        "            \"image_epoch_interpolated\": np.nan,\n",
        "            \"interpolated_time\": \"\",\n",
        "            \"numReactions\": \"\",\n",
        "            \"numLikes\": \"\",\n",
        "            \"numInterests\": \"\",\n",
        "            \"numAppreciates\": \"\",\n",
        "            \"numEntertains\": \"\",\n",
        "            \"numEmpathys\": \"\",\n",
        "            \"numPraises\": \"\",\n",
        "            \"numComments\": \"\",\n",
        "            \"numShares\": \"\",\n",
        "            \"type\": \"Share No Comment\"\n",
        "        })\n",
        "\n",
        "    # Case 2: Reshare WITH COMMENT\n",
        "    elif post[\"resharedUpdate_id\"] and not post[\"header\"] and post.get(\"type\") is None:\n",
        "        post[\"type\"] = \"Share With Comment\"\n",
        "\n",
        "    # Case 3: Original post\n",
        "    elif (not post[\"resharedUpdate_id\"]) and post.get(\"type\") is None:\n",
        "        post[\"type\"] = \"Original\"\n",
        "\n",
        "    # === Clean up unused fields ===\n",
        "    for field in [\"socialDetail\", \"sd_left\", \"sd_right\", \"header_profile_id\", \"company_id\", \"file_segment\", \"header\"]:\n",
        "        if field in post:\n",
        "            del post[field]\n",
        "\n",
        "print(\"‚úÖ Posts enriched and tagged\")"
      ],
      "metadata": {
        "id": "4lRg_zbdeeDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üìÖ **Interpolating Post Timestamps (image_epoch)**\n",
        "\n",
        "### **Why is this step needed?**\n",
        "\n",
        "When LinkedIn displays a post, it sometimes contains an embedded image (like a profile photo, attachment, or inline image). Conveniently, the LinkedIn image URLs include an *epoch timestamp* ‚Äî which tells us roughly when the image was uploaded (and thus, when the post was made).\n",
        "\n",
        "But:\n",
        "\n",
        "- Not every post contains an image.\n",
        "- If there's no image, we don't get an epoch timestamp from that post.\n",
        "- However, LinkedIn post IDs (`entity_id`) are **sequential** ‚Üí newer posts have higher IDs.\n",
        "\n",
        "So we can **interpolate the missing timestamps** based on the `entity_id` order!\n",
        "\n",
        "---\n",
        "\n",
        "### üïµÔ∏è **How this works**\n",
        "\n",
        "1Ô∏è‚É£ If the post has an image with a valid timestamp ‚Üí use it.  \n",
        "2Ô∏è‚É£ If the post is missing a timestamp:\n",
        "   - Find the nearest posts *before and after* it (based on entity_id) that DO have valid timestamps.\n",
        "   - Interpolate a timestamp between them (linear interpolation).\n",
        "3Ô∏è‚É£ If the post is at the very start (no \"before\" posts yet), we extrapolate **backward** using the first known slope.\n",
        "4Ô∏è‚É£ If the post is at the very end (no \"after\" posts), we extrapolate **forward**.\n",
        "\n",
        "---\n",
        "\n",
        "### üóÇÔ∏è **What columns are added?**\n",
        "\n",
        "‚úÖ `image_epoch_interpolated` ‚Üí A timestamp for every post (either real or interpolated).  \n",
        "‚úÖ `interpolated_time` ‚Üí A human-readable datetime version (YYYY-MM-DD HH:mm:ss).\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **Why this is safe**\n",
        "\n",
        "- LinkedIn `entity_id` is monotonically increasing ‚Üí post order is preserved.\n",
        "- The gaps between posts are small enough that interpolated timestamps are very accurate (a few seconds/minutes of uncertainty at most).\n",
        "- This allows us to **plot time trends**, **analyze posting patterns**, and **compare gaps between posts** even for those that lacked an image.\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è **In this code cell:**\n",
        "\n",
        "```python\n",
        "df = pd.DataFrame(posts)\n",
        "df = df.sort_values(by=\"entity_id\", ascending=False)\n",
        "df = interpolate_epochs(df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "At the end of this step, every post will have a timestamp ‚Äî either directly from LinkedIn or inferred.\n"
      ],
      "metadata": {
        "id": "jCWJ7cGkJqRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Interpolate image_epoch ===\n",
        "\n",
        "def interpolate_epochs(df):\n",
        "    \"\"\"\n",
        "    Interpolates missing 'image_epoch' values for posts.\n",
        "\n",
        "    Why?\n",
        "    - Some LinkedIn posts do not contain an image (and thus no epoch timestamp from the image URL).\n",
        "    - But 'entity_id' is a sequential ID (higher entity_id = newer post).\n",
        "    - We can infer missing timestamps by interpolating between known points.\n",
        "\n",
        "    What it does:\n",
        "    - Fills in 'image_epoch_interpolated' for all posts.\n",
        "    - Also adds a human-readable 'interpolated_time' column.\n",
        "\n",
        "    Approach:\n",
        "    - Forward interpolation between known image_epoch values.\n",
        "    - Handles leading gaps (early records) with extrapolation.\n",
        "    \"\"\"\n",
        "    known_points = []  # (entity_id, image_epoch) pairs where we know the timestamp\n",
        "    early_records = [] # Posts at the start with no known earlier points\n",
        "    slope = None       # Slope of entity_id ‚Üí epoch (used for extrapolation)\n",
        "    first_known_entity_id = None\n",
        "    first_known_epoch = None\n",
        "\n",
        "    # Ensure image_epoch is numeric\n",
        "    df[\"image_epoch\"] = pd.to_numeric(df[\"image_epoch\"], errors=\"coerce\")\n",
        "\n",
        "    # Go through each post\n",
        "    for idx, row in df.iterrows():\n",
        "        entity_id_numeric = int(row[\"entity_id\"])\n",
        "        current_epoch = row[\"image_epoch\"]\n",
        "\n",
        "        if pd.notna(current_epoch):\n",
        "            # We have a known epoch ‚Üí record it\n",
        "            known_points.append((entity_id_numeric, float(current_epoch)))\n",
        "            df.at[idx, \"image_epoch_interpolated\"] = current_epoch\n",
        "\n",
        "            # If we now have at least 2 known points, we can compute slope\n",
        "            if len(known_points) >= 2:\n",
        "                eid_before, epoch_before = known_points[-2]\n",
        "                eid_after, epoch_after = known_points[-1]\n",
        "                slope = (epoch_after - epoch_before) / (eid_after - eid_before)\n",
        "\n",
        "                # Save first known point (for leading gaps)\n",
        "                if first_known_entity_id is None:\n",
        "                    first_known_entity_id = eid_before\n",
        "                    first_known_epoch = epoch_before\n",
        "\n",
        "                # Fill in early records (those that came BEFORE first known point)\n",
        "                for early_idx, early_eid in early_records:\n",
        "                    extrapolated_epoch = first_known_epoch - slope * (first_known_entity_id - early_eid)\n",
        "                    df.at[early_idx, \"image_epoch_interpolated\"] = int(round(extrapolated_epoch))\n",
        "                    df.at[early_idx, \"interpolated_time\"] = datetime.fromtimestamp(df.at[early_idx, \"image_epoch_interpolated\"] / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "                early_records.clear()\n",
        "\n",
        "        else:\n",
        "            # This post is missing image_epoch ‚Äî interpolate\n",
        "            if len(known_points) >= 2:\n",
        "                # Find known points before and after this post\n",
        "                before = [pt for pt in known_points if pt[0] < entity_id_numeric]\n",
        "                after = [pt for pt in known_points if pt[0] > entity_id_numeric]\n",
        "\n",
        "                # Case 1: Both before and after points exist ‚Äî true interpolation\n",
        "                if before and after:\n",
        "                    eid_before, epoch_before = max(before, key=lambda x: x[0])\n",
        "                    eid_after, epoch_after = min(after, key=lambda x: x[0])\n",
        "                    local_slope = (epoch_after - epoch_before) / (eid_after - eid_before)\n",
        "                    interpolated_epoch = epoch_before + local_slope * (entity_id_numeric - eid_before)\n",
        "\n",
        "                # Case 2: Only before points ‚Äî extrapolate forward\n",
        "                elif before:\n",
        "                    if len(before) >= 2:\n",
        "                        b1, b2 = before[-2:]\n",
        "                        local_slope = (b2[1] - b1[1]) / (b2[0] - b1[0])\n",
        "                    else:\n",
        "                        local_slope = slope or 0\n",
        "\n",
        "                    eid_before, epoch_before = max(before, key=lambda x: x[0])\n",
        "                    interpolated_epoch = epoch_before + local_slope * (entity_id_numeric - eid_before)\n",
        "\n",
        "                # Case 3: Only after points ‚Äî extrapolate backward\n",
        "                elif after:\n",
        "                    if len(after) >= 2:\n",
        "                        a1, a2 = after[:2]\n",
        "                        local_slope = (a2[1] - a1[1]) / (a2[0] - a1[0])\n",
        "                    else:\n",
        "                        local_slope = slope or 0\n",
        "\n",
        "                    eid_after, epoch_after = min(after, key=lambda x: x[0])\n",
        "                    interpolated_epoch = epoch_after - local_slope * (eid_after - entity_id_numeric)\n",
        "\n",
        "                else:\n",
        "                    # No known points at all ‚Äî can't interpolate\n",
        "                    interpolated_epoch = 0\n",
        "\n",
        "                df.at[idx, \"image_epoch_interpolated\"] = int(round(interpolated_epoch))\n",
        "\n",
        "            else:\n",
        "                # Not enough known points yet ‚Äî store this for later extrapolation\n",
        "                early_records.append((idx, entity_id_numeric))\n",
        "                df.at[idx, \"image_epoch_interpolated\"] = np.nan\n",
        "\n",
        "        # Add human-readable time column\n",
        "        if pd.notna(df.at[idx, \"image_epoch_interpolated\"]):\n",
        "            df.at[idx, \"interpolated_time\"] = datetime.fromtimestamp(df.at[idx, \"image_epoch_interpolated\"] / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# === Run interpolation ===\n",
        "\n",
        "# Prepare dataframe\n",
        "df = pd.DataFrame(posts)\n",
        "df = df.sort_values(by=\"entity_id\", ascending=False)\n",
        "\n",
        "# Run interpolation\n",
        "df = interpolate_epochs(df)\n",
        "print(\"‚úÖ Interpolation complete\")"
      ],
      "metadata": {
        "id": "1WtiIqdpeitC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì§ **Output: Save Data to Excel**\n",
        "\n",
        "In this step, we save our **cleaned and structured post data** to an Excel file.  \n",
        "This gives us a permanent snapshot of all extracted post information ‚Äî ready for:\n",
        "\n",
        "‚úÖ Manual exploration in Excel or Google Sheets  \n",
        "‚úÖ Sharing with others  \n",
        "‚úÖ Further analysis (in Python, R, etc.)  \n",
        "‚úÖ Feeding into the next **statistics section** of this notebook  \n",
        "\n",
        "---\n",
        "\n",
        "### ‚ú® **Why save now?**\n",
        "\n",
        "- This export is a **full post-level dataset** ‚Äî no summarisation yet.\n",
        "- It captures:\n",
        "  - Who posted what\n",
        "  - When they posted it (real or interpolated time)\n",
        "  - Engagement metrics\n",
        "  - Links between shared posts\n",
        "  - Post types (Original / Share With Comment / Share No Comment)\n",
        "\n",
        "---\n",
        "\n",
        "### üóÇÔ∏è **What's in the Excel file?**\n",
        "\n",
        "| Column | Description |\n",
        "|--------|-------------|\n",
        "| `entity_id` | Unique ID for this post |\n",
        "| `resharedUpdate_id` | If this post is a share, the ID of the original post |\n",
        "| `post_text` | The full text of the post |\n",
        "| `actor_description` | Bio/description of the author |\n",
        "| `actor_name` | Name of the author |\n",
        "| `actor_profile` | Profile ID string |\n",
        "| `actor_backendUrn` | LinkedIn internal backend URN |\n",
        "| `share_url` | Public URL to the post (if available) |\n",
        "| `type` | Type of post (Original / Share With Comment / Share No Comment) |\n",
        "| `numReactions` | Total number of reactions |\n",
        "| `numLikes` | Count of likes |\n",
        "| `numInterests` | Count of \"Interest\" reactions |\n",
        "| `numAppreciates` | Count of \"Appreciation\" reactions |\n",
        "| `numEntertains` | Count of \"Entertainment\" reactions |\n",
        "| `numEmpathys` | Count of \"Empathy\" reactions |\n",
        "| `numPraises` | Count of \"Praise\" reactions |\n",
        "| `numComments` | Number of comments |\n",
        "| `numShares` | Number of shares |\n",
        "| `image_epoch` | Raw timestamp extracted from image URL (if available) |\n",
        "| `image_epoch_interpolated` | Interpolated timestamp (filled for all posts) |\n",
        "| `interpolated_time` | Human-readable post time (YYYY-MM-DD HH:MM:SS) |\n",
        "\n",
        "---\n",
        "\n",
        "### üìù **Why this is useful**\n",
        "\n",
        "- This file provides a **flat, consistent source of truth**.\n",
        "- You can use this to:\n",
        "  - Drive additional custom analysis.\n",
        "  - Build charts and reports.\n",
        "  - Cross-reference posts in LinkedIn.\n",
        "  - Select variables to feed into your **stats & trends** section later in the notebook.\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ **How it works**\n",
        "\n",
        "1Ô∏è‚É£ Ensures `entity_id` and `resharedUpdate_id` are stored as strings (for consistent export).  \n",
        "2Ô∏è‚É£ Defines which **columns** to include in the Excel output.  \n",
        "3Ô∏è‚É£ Uses `pd.ExcelWriter` to write the data to an `.xlsx` file using the XlsxWriter engine.  \n",
        "4Ô∏è‚É£ Saves the file to:  \n",
        "```\n",
        "/content/drive/MyDrive/LinkedInData/linkedin_all_data.xlsx\n",
        "```\n",
        "This path/filename can be altered if you choose.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ When this runs successfully, you'll see:  \n",
        "```\n",
        "‚úÖ Excel file created: /content/drive/MyDrive/LinkedInData/linkedin_all_data.xlsx\n",
        "```\n",
        "\n",
        "This file can now be used anywhere. It can even be used as the **input for the next analysis section** of this tutorial if you wish!"
      ],
      "metadata": {
        "id": "_G2797grKP_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Output: Save to Excel ===\n",
        "\n",
        "# Why?\n",
        "# We want an easy-to-use **structured output** ‚Äî so we save it as an Excel file.\n",
        "# This makes it simple to:\n",
        "# - Explore in Excel / Google Sheets\n",
        "# - Do further analysis\n",
        "# - Share with others\n",
        "\n",
        "# First: ensure these key fields are strings (for consistent export)\n",
        "df[\"entity_id\"] = df[\"entity_id\"].astype(str)\n",
        "df[\"resharedUpdate_id\"] = df[\"resharedUpdate_id\"].astype(str)\n",
        "\n",
        "# Output path ‚Äî adjust if you want a different location\n",
        "output_path = \"/content/drive/MyDrive/LinkedInData/linkedin_all_data.xlsx\"\n",
        "\n",
        "# === What we include in the output ===\n",
        "\n",
        "# These columns give us:\n",
        "# - Post identity\n",
        "# - Who posted it\n",
        "# - Content\n",
        "# - Engagement stats\n",
        "# - Timing (including interpolated time)\n",
        "# - Post type (original / share)\n",
        "\n",
        "show_columns = [\n",
        "    \"entity_id\",                 # Unique post ID\n",
        "    \"resharedUpdate_id\",         # If this is a share, what post was shared\n",
        "    \"post_text\",                 # The text of the post\n",
        "    \"actor_description\",         # Author bio/description\n",
        "    \"actor_name\",                # Author name\n",
        "    \"actor_profile\",             # Profile ID\n",
        "    \"actor_backendUrn\",          # LinkedIn internal ID\n",
        "    \"share_url\",                 # Public URL to this post\n",
        "    \"type\",                      # Original, Share with comment, Share no comment\n",
        "    \"numReactions\",              # Total reactions\n",
        "    \"numLikes\",                  # Like count\n",
        "    \"numInterests\",              # Interest reactions\n",
        "    \"numAppreciates\",            # Appreciation reactions\n",
        "    \"numEntertains\",             # Entertainment reactions\n",
        "    \"numEmpathys\",               # Empathy reactions\n",
        "    \"numPraises\",                # Praise reactions\n",
        "    \"numComments\",               # Number of comments\n",
        "    \"numShares\",                 # Number of shares\n",
        "    \"image_epoch\",               # Raw timestamp (from image URL) ‚Äî where available\n",
        "    \"image_epoch_interpolated\",  # Interpolated timestamp (for all posts)\n",
        "    \"interpolated_time\"          # Human-readable time (yyyy-mm-dd HH:MM:SS)\n",
        "]\n",
        "\n",
        "# === Save the file ===\n",
        "\n",
        "# We use ExcelWriter with XlsxWriter engine (produces a modern .xlsx)\n",
        "with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
        "    df.to_excel(writer, sheet_name='Post_Data', index=False, columns=show_columns)\n",
        "\n",
        "print(\"‚úÖ Excel file created:\", output_path)"
      ],
      "metadata": {
        "id": "wrVPzQGjenGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìà **Final Step: Generate and Export Post Statistics**\n",
        "\n",
        "---\n",
        "\n",
        "Now that we have a full, structured dataset of posts, we can generate some **summary statistics** ‚Äî this is where the insights start to emerge!\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ **What this section does**\n",
        "\n",
        "1Ô∏è‚É£ **Loads the cleaned Excel data** (optional if you've already run the notebook end-to-end).  \n",
        "2Ô∏è‚É£ **Parses timestamps** ‚Äî so we can analyze posts over time.  \n",
        "3Ô∏è‚É£ **Filters for a single author** and only valid post types:  \n",
        "   - `Original`  \n",
        "   - `Share With Comment`  \n",
        "   - `Share No Comment`  \n",
        "\n",
        "4Ô∏è‚É£ **Calculates core metrics**:  \n",
        "   - Posting date, week, year  \n",
        "   - Word count  \n",
        "   - Character count  \n",
        "   - Emoji usage  \n",
        "   - Hashtag usage  \n",
        "   - Time gaps between posts (for burst analysis)  \n",
        "\n",
        "5Ô∏è‚É£ **Creates the following summary tables**:\n",
        "- **Daily Stats** ‚Üí Posts & average word count per day\n",
        "- **Weekly Stats** ‚Üí Posts & average word count per week\n",
        "- **Day of Week Stats** ‚Üí Posting patterns by day of the week\n",
        "- **Overall Metrics** ‚Üí Totals and averages, split by post type\n",
        "\n",
        "6Ô∏è‚É£ **Exports all of this to an Excel file** (`linkedin_combined_stats.xlsx`) ‚Äî with multiple sheets:\n",
        "- `Daily_Stats`\n",
        "- `Weekly_Stats`\n",
        "- `DayOfWeek_Stats`\n",
        "- `Overall_Stats`\n",
        "- `Enhanced_Post_Data` ‚Üí Full post-level data with new metrics\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **Why this is useful**\n",
        "\n",
        "These stats allow you to explore questions like:\n",
        "\n",
        "- **How frequently does this person post?**\n",
        "- **Do they post in bursts, or consistently?**\n",
        "- **How long are their posts on average?**\n",
        "- **What days of the week are they most active?**\n",
        "- **Are they producing \"original\" content or mostly shares?**\n",
        "- **Has their activity changed over time?**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úèÔ∏è **Notes & Customization**\n",
        "\n",
        "- You must replace the target **`actor_profile`** with any profile ID you want to analyze.\n",
        "\n",
        "  This can be found in the Excel file output in the previous step.\n",
        "\n",
        "  (This allows re-running the same analysis for multiple creators ‚Äî just swap the ID.)\n",
        "  ```python\n",
        "filtered_df = df.loc[\n",
        "    ((df[\"type\"] == \"Original\") |\n",
        "     (df[\"type\"] == \"Share With Comment\") |\n",
        "     (df[\"type\"] == \"Share No Comment\")) &\n",
        "    (df[\"actor_profile\"] == \"ACoAABDoFkIBu5s4sIdD-WTas39dsfSNq-XYDF\")  # <-- Replace with your author!\n",
        "].copy()\n",
        "  ```\n",
        "\n",
        "- You can also easily add more metrics here:\n",
        "  - Average time gap between posts  \n",
        "  - Post sentiment (using an LLM!)  \n",
        "  - Engagement per post type  \n",
        "  - And more...\n",
        "\n",
        "---\n",
        "\n",
        "### üìÇ **Output Files**\n",
        "\n",
        "The final Excel file will be saved to:\n",
        "\n",
        "```python\n",
        "output_path = \"/content/drive/MyDrive/LinkedInData/linkedin_combined_stats.xlsx\"\n",
        "```\n",
        "\n",
        "This can be changed if you wish.\n",
        "\n",
        "---\n",
        "\n",
        "### üéÅ **Why this design?**\n",
        "\n",
        "- **Separation of concerns**:\n",
        "  - The earlier \"all_data\" export is a **raw snapshot** of posts.\n",
        "  - This step creates **derived stats** based on a target author and post type.\n",
        "\n",
        "- **Flexibility**:\n",
        "  - You can experiment and iterate on metrics without having to re-extract all HAR data.\n",
        "\n",
        "- **Reproducibility**:\n",
        "  - You can run this for any LinkedIn user whose posts you've captured.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ After this step, you'll have a complete dataset, ready for:\n",
        "\n",
        "üìä Visualisation  \n",
        "üß† Interpretation  \n",
        "ü§ñ LLM analysis (e.g. \"Summarize this author's tone\")  \n",
        "üìà Comparative studies across creators  "
      ],
      "metadata": {
        "id": "eAvymBYVLFvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load and Prepare Data for Analysis ===\n",
        "\n",
        "# OPTIONAL: Load from Excel file created in previous step\n",
        "\"\"\"\n",
        "input_path = \"/content/drive/MyDrive/LinkedInData/linkedin_all_data.xlsx\"\n",
        "df = pd.read_excel(input_path, sheet_name='Post_Data')\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(df)} posts from Excel\")\n",
        "\"\"\"\n",
        "\n",
        "# === Parse datetime ===\n",
        "df['image_datetime'] = pd.to_datetime(df['interpolated_time'], errors='coerce')\n",
        "\n",
        "# Drop rows where datetime is missing\n",
        "df = df.dropna(subset=['image_datetime'])\n",
        "\n",
        "# === Filter: Target Author + Valid Post Types ===\n",
        "\n",
        "# Why?\n",
        "# - We want to focus on ONE author (replace with your target actor_profile)\n",
        "# - Only posts of type Original / Share With Comment / Share No Comment\n",
        "\n",
        "filtered_df = df.loc[\n",
        "    ((df[\"type\"] == \"Original\") |\n",
        "     (df[\"type\"] == \"Share With Comment\") |\n",
        "     (df[\"type\"] == \"Share No Comment\")) &\n",
        "    (df[\"actor_profile\"] == \"ACoAABDoFkIBu5s4sIdD-WTas39dsfSNq-XYDF\")  # <-- Replace with your author!\n",
        "].copy()\n",
        "\n",
        "# === Core Metrics ===\n",
        "\n",
        "# Extract useful time dimensions\n",
        "filtered_df['date'] = filtered_df['image_datetime'].dt.date\n",
        "filtered_df['week'] = filtered_df['image_datetime'].dt.isocalendar().week\n",
        "filtered_df['year'] = filtered_df['image_datetime'].dt.year\n",
        "\n",
        "# Word count\n",
        "filtered_df['word_count'] = filtered_df['post_text'].fillna(\"\").apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# === Additional Metrics ===\n",
        "\n",
        "filtered_df['day_of_week'] = filtered_df['image_datetime'].dt.day_name()\n",
        "filtered_df['char_count'] = filtered_df['post_text'].fillna(\"\").apply(len)\n",
        "filtered_df['emoji_count'] = filtered_df['post_text'].fillna(\"\").apply(lambda x: len(re.findall(r'[^\\w\\s,]', x)))\n",
        "filtered_df['hashtag_count'] = filtered_df['post_text'].fillna(\"\").apply(lambda x: len(re.findall(r'#\\w+', x)))\n",
        "\n",
        "# Calculate time gaps between posts\n",
        "filtered_df = filtered_df.sort_values('image_datetime')\n",
        "filtered_df['post_gap_seconds'] = filtered_df['image_datetime'].diff().dt.total_seconds().fillna(0)\n",
        "filtered_df['post_gap_minutes'] = filtered_df['post_gap_seconds'] / 60\n",
        "filtered_df['post_gap_hours'] = filtered_df['post_gap_seconds'] / 3600\n",
        "\n",
        "# === Daily Stats ===\n",
        "\n",
        "posts_pivot = filtered_df.pivot_table(\n",
        "    index='date',\n",
        "    columns='type',\n",
        "    values='entity_id',\n",
        "    aggfunc='count',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "words_pivot = filtered_df.pivot_table(\n",
        "    index='date',\n",
        "    columns='type',\n",
        "    values='word_count',\n",
        "    aggfunc='mean',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "daily_stats = posts_pivot.merge(words_pivot, on='date', suffixes=(' Count', ' Avg Words'))\n",
        "\n",
        "# === Weekly Stats ===\n",
        "\n",
        "weekly_posts_pivot = filtered_df.pivot_table(\n",
        "    index=['year', 'week'],\n",
        "    columns='type',\n",
        "    values='entity_id',\n",
        "    aggfunc='count',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "weekly_words_pivot = filtered_df.pivot_table(\n",
        "    index=['year', 'week'],\n",
        "    columns='type',\n",
        "    values='word_count',\n",
        "    aggfunc='mean',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "weekly_stats = weekly_posts_pivot.merge(weekly_words_pivot, on=['year', 'week'], suffixes=(' Count', ' Avg Words'))\n",
        "\n",
        "# === Day of Week Stats ===\n",
        "\n",
        "day_of_week_posts_pivot = filtered_df.pivot_table(\n",
        "    index='day_of_week',\n",
        "    columns='type',\n",
        "    values='entity_id',\n",
        "    aggfunc='count',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "day_of_week_words_pivot = filtered_df.pivot_table(\n",
        "    index='day_of_week',\n",
        "    columns='type',\n",
        "    values='word_count',\n",
        "    aggfunc='mean',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "day_of_week_stats = day_of_week_posts_pivot.merge(day_of_week_words_pivot, on='day_of_week', suffixes=(' Count', ' Avg Words'))\n",
        "\n",
        "# === Overall Metrics (by Type) ===\n",
        "\n",
        "overall_by_type = filtered_df.groupby('type').agg(\n",
        "    total_posts=('post_text', 'count'),\n",
        "    unique_post_days=('date', 'nunique'),\n",
        "    average_posts_per_day=('date', lambda x: x.value_counts().mean()),\n",
        "    average_word_count=('word_count', 'mean'),\n",
        "    max_word_count=('word_count', 'max'),\n",
        "    min_word_count=('word_count', 'min'),\n",
        "    most_active_day=('date', lambda x: x.value_counts().idxmax()),\n",
        "    most_active_day_count=('date', lambda x: x.value_counts().max())\n",
        ").reset_index()\n",
        "\n",
        "# === Export All Stats to Excel ===\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/LinkedInData/linkedin_combined_stats.xlsx\"\n",
        "\n",
        "with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
        "    # Save daily stats\n",
        "    daily_stats.to_excel(writer, sheet_name='Daily_Stats', index=False)\n",
        "\n",
        "    # Save weekly stats\n",
        "    weekly_stats.to_excel(writer, sheet_name='Weekly_Stats', index=False)\n",
        "\n",
        "    # Save day-of-week stats\n",
        "    day_of_week_stats.to_excel(writer, sheet_name='DayOfWeek_Stats', index=False)\n",
        "\n",
        "    # Save overall metrics by post type\n",
        "    overall_by_type.to_excel(writer, sheet_name='Overall_Stats', index=False)\n",
        "\n",
        "    # Save FULL enhanced post-level data\n",
        "    enhanced_columns = [\n",
        "        \"entity_id\",\n",
        "        \"resharedUpdate_id\",\n",
        "        \"post_text\",\n",
        "        \"actor_description\",\n",
        "        \"actor_name\",\n",
        "        \"actor_profile\",\n",
        "        \"actor_backendUrn\",\n",
        "        \"share_url\",\n",
        "        \"type\",\n",
        "        \"numReactions\",\n",
        "        \"numLikes\",\n",
        "        \"numInterests\",\n",
        "        \"numAppreciates\",\n",
        "        \"numEntertains\",\n",
        "        \"numEmpathys\",\n",
        "        \"numPraises\",\n",
        "        \"numComments\",\n",
        "        \"numShares\",\n",
        "        \"image_epoch\",\n",
        "        \"image_epoch_interpolated\",\n",
        "        \"interpolated_time\",\n",
        "        \"date\",\n",
        "        \"week\",\n",
        "        \"year\",\n",
        "        \"word_count\",\n",
        "        \"day_of_week\",\n",
        "        \"char_count\",\n",
        "        \"emoji_count\",\n",
        "        \"hashtag_count\",\n",
        "        \"post_gap_seconds\",\n",
        "        \"post_gap_minutes\",\n",
        "        \"post_gap_hours\"\n",
        "    ]\n",
        "\n",
        "    filtered_df.to_excel(writer, sheet_name='Enhanced_Post_Data', index=False, columns=enhanced_columns)\n",
        "\n",
        "print(\"‚úÖ Excel file created:\", output_path)"
      ],
      "metadata": {
        "id": "G1m-00NWxMrk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}